{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pipeline import (\n",
    "    load_config,\n",
    "    SBERTEmbedder,\n",
    "    BERTEmbedder,\n",
    "    DataAccessLayer,\n",
    "    concatenate_columns,\n",
    "    divide_dataset,\n",
    "    dataset_subset,\n",
    "    vectorize_and_save,\n",
    "    match_compositions,\n",
    "    result_dist\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from losses import get_loss_function\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search PoC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration management\n",
    "CONFIG_PATH = 'config.json'\n",
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas display setting management section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.float_format', lambda x: f'{x:.2f}') # Format float display to 2 decimal places\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.reset_option('display.float_format')\n",
    "#pd.reset_option('display.max_rows')\n",
    "#pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dal = DataAccessLayer(config['data_path'])\n",
    "dataset = dal.load_data()\n",
    "\n",
    "qa = pd.read_csv(config[\"qa_path\"])\n",
    "\n",
    "canonical_cols_inference = ['CAN_ID', 'CAN_Title', 'CAN_Writers_Formatted']\n",
    "matched_cols_inference = ['CAN_ID', 'MATCHED_Title', 'MATCHED_Writer_1']\n",
    "\n",
    "canonical_df, matched_df = divide_dataset(dataset, canonical_cols_inference, matched_cols_inference)\n",
    "\n",
    "matched_subset = dataset_subset(matched_df, sample_size=config['sample_size'], seed=config['random_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedder = BERTEmbedder(model_name=config['bert_model_name'], batch_size=64)  # Adjust batch size based on your GPU memory\n",
    "\n",
    "# Vectorize canonical dataset\n",
    "bert_canonical_vectors_path = config['bert_canonical_vectors_path']\n",
    "if not os.path.exists(bert_canonical_vectors_path):\n",
    "    canonical_vectors = vectorize_and_save(canonical_df, bert_embedder, bert_canonical_vectors_path, dal) # Vectorise all canonical compoistions\n",
    "else:\n",
    "    canonical_vectors = dal.load_vectors(bert_canonical_vectors_path)\n",
    "\n",
    "results_df = match_compositions(matched_subset, canonical_vectors, bert_embedder, canonical_df)\n",
    "results_df.to_csv(config['results_path'], index=False)\n",
    "logger.info(\"Matching completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(config['results_path'])\n",
    "results['Correct_Match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['Correct_Match', 'Similarity_Score'], ascending=[False, False], inplace=True)\n",
    "match_mask = (results['Correct_Match'] == True) & (results['Similarity_Score'] < 1) & ~(results['CAN_Title'].str.contains('pump'))\n",
    "\n",
    "results[match_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick QA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = qa['MATCHED_Title'].str.lower().str.contains('pull up')\n",
    "qa[mask].drop_duplicates('MATCHED_Comp').sort_values('CAN_Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pre-trained SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained SBERT model\n",
    "sbert_embedder = SBERTEmbedder(model_name=config['sbert_model_name'], batch_size=config['batch_size'])  # Adjust batch size based on your GPU memory\n",
    "\n",
    "# Vectorize canonical dataset\n",
    "s_bert_canonical_vectors_path = config['s_bert_canonical_vectors_path']\n",
    "if not os.path.exists(s_bert_canonical_vectors_path):\n",
    "    canonical_vectors = vectorize_and_save(canonical_df, sbert_embedder, s_bert_canonical_vectors_path, dal) # Vectorise all canonical compoistions\n",
    "else:\n",
    "    canonical_vectors = dal.load_vectors(s_bert_canonical_vectors_path)\n",
    "\n",
    "results_df = match_compositions(matched_subset, canonical_vectors, sbert_embedder, canonical_df)\n",
    "results_df.to_csv(config['results_path'], index=False)\n",
    "logger.info(\"Matching completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(config['results_path'])\n",
    "results['Correct_Match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['Correct_Match', 'Similarity_Score'], ascending=[False, False], inplace=True)\n",
    "match_mask = (results['Correct_Match'] == True) & (results['Similarity_Score'] < 1) & ~(results['CAN_Title'].str.contains('pump'))\n",
    "\n",
    "results[match_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick QA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = qa['MATCHED_Title'].str.lower().str.contains('pull up')\n",
    "qa[mask].drop_duplicates('MATCHED_Comp').sort_values('CAN_Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and evaluation\n",
    "dataset = dataset_subset(dataset, sample_size=1000, seed=27) #move to params\n",
    "#train_df, eval_df = split_dataset(dataset, sort_column='CAN_ID', random_state=config['random_state'], fine_tuning_ratio=config['fine_tuning_ratio'])\n",
    "\n",
    "canonical_cols_training = ['CAN_Title', 'CAN_Writers_Formatted']\n",
    "matched_cols_training = ['MATCHED_Title', 'MATCHED_Writer_1']\n",
    "\n",
    "canonical_texts, matched_texts = divide_dataset(dataset, canonical_cols_training, matched_cols_training)\n",
    "\n",
    "canonical_texts = concatenate_columns(canonical_texts, ['CAN_Title', 'CAN_Writers_Formatted'])\n",
    "matched_texts = concatenate_columns(matched_texts, ['MATCHED_Title', 'MATCHED_Writer_1'])\n",
    "\n",
    "training_df = pd.concat([canonical_texts, matched_texts], axis=1).reset_index(drop=True) # SentenceTransformerTrainer seems to require df w/ the default index\n",
    "training_dataset = Dataset.from_pandas(training_df)\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model = sbert_embedder.model\n",
    "\n",
    "# Fine-tune the model\n",
    "# 4. Define a loss function\n",
    "loss = get_loss_function(model)\n",
    "\n",
    "# 5. Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "output_dir=config['training_output_dir']\n",
    "# Optional training parameters:\n",
    ")\n",
    "\n",
    "# 6. Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "model=model,\n",
    "args=args,\n",
    "train_dataset=training_dataset,\n",
    "#eval_dataset=eval_dataset,\n",
    "loss=loss,\n",
    "#evaluator=dev_evaluator\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save the trained model\n",
    "model.save_pretrained(config['fine_tuned_model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned SBERT model\n",
    "sbert_embedder.model = SentenceTransformer(config['fine_tuned_model_name'])  # Adjust batch size based on your GPU memory\n",
    "\n",
    "# Vectorize canonical dataset\n",
    "fine_tuned_s_bert_canonical_vectors_path = config['fine_tuned_s_bert_canonical_vectors_path']\n",
    "if not os.path.exists(fine_tuned_s_bert_canonical_vectors_path):\n",
    "    canonical_vectors = vectorize_and_save(canonical_df, sbert_embedder, fine_tuned_s_bert_canonical_vectors_path, dal) # Vectorise all canonical compoistions\n",
    "else:\n",
    "    canonical_vectors = dal.load_vectors(fine_tuned_s_bert_canonical_vectors_path)\n",
    "\n",
    "results_df = match_compositions(matched_subset, canonical_vectors, sbert_embedder, canonical_df)\n",
    "results_df.to_csv(config['results_path'], index=False)\n",
    "logger.info(\"Matching completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune SBERT Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(config['results_path'])\n",
    "results['Correct_Match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['Correct_Match', 'Similarity_Score'], ascending=[False, False], inplace=True)\n",
    "match_mask = (results['Correct_Match'] == True) & (results['Similarity_Score'] < 1) & ~(results['CAN_Title'].str.contains('pump'))\n",
    "\n",
    "results[match_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick QA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = qa['MATCHED_Title'].str.lower().str.contains('pull up')\n",
    "qa[mask].drop_duplicates('MATCHED_Comp').sort_values('CAN_Title')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
